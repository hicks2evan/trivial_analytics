{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.0 64-bit",
   "display_name": "Python 3.6.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "05ece30799c2dcdac4c13b3af20453da19de8df0d9a1de52cff7e0b6e1e82bdd"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Trivial Analytics\n",
    "The purpose of this post is to try to use some data analytics to answer a question that came up in a conversation between me and my trivia teammates. Before the coronavirus put our favorite bar trivia night on hold, my friends and I had a ritual of appearing there every Wednesday night at 7pm and answering 8 rounds of trivia questions on a variety of [geek pop-culture subjects](https://www.geekswhodrink.com/). The question that arose at our table was along these lines:\n",
    "\n",
    "> What would you give to know the 10 most important topics to study for bar trivia?\n",
    "\n",
    "It's pretty natural to start to try to answer this question with data. For our particular trivia game, if we had the data it would be great to know which *things* appear the most in questions and answers. Knowing, for example, that Beyonce is 10% more likely to appear in an audio round than Bruno Mars is a pretty critical piece of information for somebody with a limited amount of time to prepare to win that sweet, sweet $20 bar cash.\n",
    "\n",
    "Extracting this type of insight is 'non-trivial', even assuming a perfect world where I had access to a data set with my bar trivia game's questions and answers. No such data set exists. But, after some sleuthing I found a reddit poster sharing a data set with 200,000+ Jeopardy! questions [here](https://www.reddit.com/r/datasets/comments/1uyd0t/200000_jeopardy_questions_in_a_json_file/). Bad news for my trivia team, I don't have the resources to crack the code our game, but maybe we can learn something by asking an amended question of the Jeopardy! data:\n",
    "\n",
    "> If you only could study 10 topics in preparation for Jeopardy!, which topics should you study?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Working with the data\n",
    "The first step was to set up the data in a way that would be easy for me to work with in this project. I spun up a mySQL database and added a `question` table to hold the question data from reddit. There were a number of columns that I imported, but mostly we care about the `question` and `answer` fields.\n",
    "\n",
    "The Juptyer notebook for this post is set up to easily connect to a local mySQL database assuming it is set up a similar way. Python can connect to mySQL using a package called `mysql.connector`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "def connectToMySQL():\n",
    "  mydb = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"service\",\n",
    "    password=\"jeopardy!\",\n",
    "    database=\"jeopardy_db\"\n",
    "  )\n",
    "  print(\"Connected.\")\n",
    "  print()\n",
    "  return mydb\n",
    "\n"
   ]
  },
  {
   "source": [
    "With a connection to the database open, you can execute normal SQL queries. Right away we are able to ask some fairly smart things if we know what we are looking for, like *show me five questions about Egypt*."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Connected.\n\nQuestion: | 'Cleopatra's Needle is a short walk from this Egyptian Temple in the Metropolitan Museum of Art'\nAnswer:   | the Temple of Dendur\n\nQuestion: | 'In 46 B.C. this Egyptian came with Caesar to Rome, where her statue was placed in the temple of Venus Genetrix'\nAnswer:   | Cleopatra\n\nQuestion: | '\"The Prince of Egypt\"\" featured Ralph Fiennes as the voice of this stubborn ruler'\"\nAnswer:   | the Pharaoh\n\nQuestion: | 'This city of east central Egypt is the southern half of the site of ancient Thebes'\nAnswer:   | Luxor\n\nQuestion: | 'A short war between Israel & Egypt & Syria in October 1973 was named for this high holiday'\nAnswer:   | Yom Kippur\n\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "mydb = connectToMySQL()\n",
    "cursor = mydb.cursor()\n",
    "cursor.execute(\"SELECT question, answer FROM question WHERE question like '%Egypt%' LIMIT 5\")\n",
    "\n",
    "for (question, answer) in cursor:\n",
    "    print(\"Question: | \" + question)\n",
    "    print(\"Answer:   | \" + answer)\n",
    "    print()\n",
    "\n",
    "cursor.close()"
   ]
  },
  {
   "source": [
    "So now we have a database set up and we can write queries to ask it smart things. Like mentioned above, however, this requires us to know what we are asking. Questions like *what 10 things should I study* won't fly because we can't write a query yet for *things* we don't know we care about. We need some way to figure out what *things* in the questions are important."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Named Entity Recognition\n",
    "So what do I mean by *thing*? One naive solution to our problem might be to just look for common appearances of certain words. For example, if \"America\" appears regularly in questions, then that might be an important country to study, right?Well, practiced trivia players know that trivia is all about going more fine-grained than that. American History may be a very important subject to study, but at the end of the day, you may need to know some specifics about Hamilton that you may gloss over if you only study American History broadly. \n",
    "\n",
    "Consider another issue of the word count solution, it may tell you that it is quite important to know about \"Alexander\", but *Alexander-who?* Alexander Hamilton and Alexander the Great might both be important, but the word count solution doesn't tell us who is *more* important.\n",
    "\n",
    "Another idea is to use the `category` of a question. That should help us get to the meat of what a question is about, but viewers of Jeopardy! will know well that the category is usually not useful, if not downright distracting. Categories like \"African Geography\" are way too broad to be useful. Meanwhile, many of the Jeapordy! categories are unique to the game, playful rhymes or word games.\n",
    "\n",
    "In general, it looks like we are trying to extract people, places, times, etc. from the questions. In Natural Language Processing (NLP) there is a name for annotating this type of information, \"Named Entity Recognition\". Fortunately there are handy Python libraries out there like [spaCy](https://spacy.io/api/entityrecognizer) that can do the heavy lifting for this.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[('The Prince of Egypt', 'WORK_OF_ART'), ('Ralph Fiennes', 'PERSON')]\n"
    }
   ],
   "source": [
    "# Reference https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "def printAnnotation(q):\n",
    "    doc = nlp(q)\n",
    "    print([(X.text, X.label_) for X in doc.ents])\n",
    "\n",
    "q = '\"The Prince of Egypt\" featured Ralph Fiennes as the voice of this stubborn ruler'\n",
    "printAnnotation(q)"
   ]
  },
  {
   "source": [
    "Here we can see spaCy was able to identidy to named entities in this question, \"The Prince of Egypt\" was labeled as a work of art (animated film, go watch it), and \"Ralph Fiennes\" was labeled as a person. This works pretty well generally, but it isn't perfect. For example, below it has decided that \"Israel & Egypt & Syria\" are an organization all-together. Hopefully these things will *come out in the wash* so to speak, but we should keep an eye out for misleading entities.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[('Israel & Egypt & Syria', 'ORG'), ('October 1973', 'DATE')]\n"
    }
   ],
   "source": [
    "q = \"A short war between Israel & Egypt & Syria in October 1973 was named for this high holiday\"\n",
    "printAnnotation(q)"
   ]
  },
  {
   "source": [
    "## Mapping Questions to Named Entities\n",
    "Next thing to do to get an idea of which named entities occur frequently, we want to map questions and answer text to named entities. For this, we need a new table to track those entities and their labels, as well as a mapping table to handle the many to many relationship of question to named_entity. For those of you who aren't familiar with the idea of using mapping tables for many to many relationships in normalized databases, check out [this post](https://www.joinfu.com/2005/12/managing-many-to-many-relationships-in-mysql-part-1/).\n",
    "\n",
    "With that in place, we can set up some templated queries using template literals in Python. Establishing these common uses up front will allow us to get some reuse out of them as we go through the remainder of the project."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to get all questions from question table with limit and offset to paginate\n",
    "get_all_questions = (\"SELECT question_id, question, answer FROM question LIMIT %s OFFSET %s\")\n",
    "\n",
    "# Queries to wipe out tables before re-seeding\n",
    "delete_mappings = \"DELETE FROM question_entity_map\"\n",
    "delete_entities = \"DELETE FROM entity\"\n",
    "\n",
    "# Queries to add named entities and mappings\n",
    "add_entity = (\"INSERT INTO entity (name, label) VALUES (%s, %s)\")\n",
    "add_mapping = (\"INSERT INTO question_entity_map (question, entity) VALUES (%s, %s)\")"
   ]
  },
  {
   "source": [
    "The code below will seed our two new tables. It goes question by question and looks for named entities that it hasn't seen before and adds them to the `entity` table. It doesn't add the entity if it is a recurrence. In any case it adds a mapping to the question in the `question_entity_map` mapping table. First it needs to clear out whatever was seeded previously. I paginated the operation so that it doesn't do one **huge** insert, but this can be controlled by tweaking the limit and offset variables.\n",
    "\n",
    "Go get a cup of coffee while this one runs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Connected.\n\nDeleting previously seeded records...\nStarting get for offset 0...\nStarting get for offset 5000...\nStarting get for offset 10000...\nStarting get for offset 15000...\nStarting get for offset 20000...\nStarting get for offset 25000...\nStarting get for offset 30000...\nStarting get for offset 35000...\nStarting get for offset 40000...\nStarting get for offset 45000...\nStarting get for offset 50000...\nStarting get for offset 55000...\nStarting get for offset 60000...\nStarting get for offset 65000...\nStarting get for offset 70000...\nStarting get for offset 75000...\nStarting get for offset 80000...\nStarting get for offset 85000...\nStarting get for offset 90000...\nStarting get for offset 95000...\nStarting get for offset 100000...\nStarting get for offset 105000...\nStarting get for offset 110000...\nStarting get for offset 115000...\nStarting get for offset 120000...\nStarting get for offset 125000...\nStarting get for offset 130000...\nStarting get for offset 135000...\nStarting get for offset 140000...\nStarting get for offset 145000...\nStarting get for offset 150000...\nStarting get for offset 155000...\nStarting get for offset 160000...\nStarting get for offset 165000...\nStarting get for offset 170000...\nStarting get for offset 175000...\nStarting get for offset 180000...\nStarting get for offset 185000...\nStarting get for offset 190000...\nStarting get for offset 195000...\nDone, closing...\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "import regex\n",
    "import unidecode\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "mydb = connectToMySQL()\n",
    "cursor = mydb.cursor()\n",
    "\n",
    "print(\"Deleting previously seeded records...\")\n",
    "cursor.execute(delete_mappings)\n",
    "cursor.execute(delete_entities)\n",
    "mydb.commit()\n",
    "\n",
    "limit = 5000\n",
    "start = 0\n",
    "end = 200000\n",
    "data_entities = dict()\n",
    "all_data_entities = dict()\n",
    "data_mappings = []\n",
    "\n",
    "for i in range(int(start/limit), int(end/limit)):\n",
    "  offset = i * limit\n",
    "  print(\"Starting get for offset \" + str(offset) + \"...\")\n",
    "  get_data = (limit, offset)\n",
    "  cursor.execute(get_all_questions, get_data)\n",
    "\n",
    "  for (question_id, question, answer) in cursor:\n",
    "    q = unidecode.unidecode(regex.sub(\"'\", \"\", question + \" \" + answer))\n",
    "    doc = nlp(q)\n",
    "    for X in doc.ents:\n",
    "        entity = X.text.lower()\n",
    "        if not entity in data_entities:\n",
    "          if not entity in all_data_entities:\n",
    "            data_entities[entity] = X.label_\n",
    "        data_mapping = (question_id, entity)\n",
    "        data_mappings.append(data_mapping)\n",
    "\n",
    "  cursor.executemany(add_entity, (list(data_entities.items())))\n",
    "  mydb.commit()\n",
    "  cursor.executemany(add_mapping, (data_mappings))\n",
    "  mydb.commit()\n",
    "\n",
    "  all_data_entities = {**all_data_entities, **data_entities} \n",
    "  data_entities.clear()\n",
    "  data_mappings = []\n",
    "\n",
    "print(\"Done, closing...\")\n",
    "cursor.close()"
   ]
  },
  {
   "source": [
    "With that in place, we should be able to query the mapping table for information about occurrences of certain named entities in questions. In fact, the entities with the highest count in the mapping table are those entities that appeared most commonly in questions. We can write a rudimentary query to try to answer our original question."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Connected.\n\n('first', 4385)\n('one', 3134)\n('2', 2453)\n('u.s.', 2210)\n('french', 1258)\n('3', 1059)\n('british', 958)\n('1', 889)\n('greek', 772)\n('american', 766)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "mydb = connectToMySQL()\n",
    "cursor = mydb.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT entity, COUNT(*) FROM question_entity_map GROUP BY entity ORDER BY COUNT(*) DESC LIMIT 10;\")\n",
    "\n",
    "for r in cursor:\n",
    "    print(str(r))\n",
    "    \n",
    "cursor.close()"
   ]
  },
  {
   "source": [
    "We're getting closer. Not too surprisingly, we see frequent occurences of what look like geographic or linguistic designations. French, American, British, and Greek. There also appear to be cardinal numbers, as well as some ordinal numbers like First and Second. Let's look into the labels for some of these frequently appearing entities. To do this, we need to be able to pull back data from `question`, but also take into account the label in the `entity` table. We can write a join to look for questions containing named entities with those labels."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join to pull data from question table and entity table linked by mapping table\n",
    "get_question_by_entity_type = (\n",
    "    \"SELECT question.question, entity.label FROM question_entity_map \" + \n",
    "    \"LEFT JOIN question ON question.question_id = question_entity_map.question \" +\n",
    "    \"LEFT JOIN entity ON question_entity_map.entity = entity.name \" +\n",
    "    \"WHERE question_entity_map.entity = '%s' LIMIT 5;\"\n",
    ")"
   ]
  },
  {
   "source": [
    "Using that we can query for various questions including entities that seem off, and see what their labels are."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Connected.\n\namerican:\n('\\'\"American poet... became known as a leader of the Beat literary movement of the 1950s\"\"\\'\"', 'NORP')\n('\\'\"One of the most original and provocative American architects working today\"\"\\'\"', 'NORP')\n(\"'Sukkot, a Jewish festival, began as a harvest celebration & was a model for this centuries-old American holiday'\", 'NORP')\n(\"'This astronomer for whom a space telescope is named is honored in the American Scientists series'\", 'NORP')\n(\"'O Canada celebrates Canada Day on this date, 3 days before a big American holiday'\", 'NORP')\n\nfirst:\n(\"'Cows regurgitate this from the first stomach to the mouth & chew it again'\", 'ORDINAL')\n(\"'Karl led the first of these Marxist organizational efforts; the second one began in 1889'\", 'ORDINAL')\n('\\'This \"Modern Girl\"\" first hit the Billboard Top 10 with \"\"Morning Train (Nine To Five)\"\"\\'\"', 'ORDINAL')\n(\"'Warhol became the manager of this Lou Reed rock group in 1965 & produced their first album'\", 'ORDINAL')\n(\"'His first act after being sworn in as president of the Confederacy was to send a peace commission to Washington, D.C.'\", 'ORDINAL')\n\none:\n(\"'In geologic time one of these, shorter than an eon, is divided into periods & subdivided into epochs'\", 'CARDINAL')\n('\\'Teri Hatcher looked \"shipshape\"\" as one of the singing \"\"mermaids\"\" who jumped on board this cruisin\\' series in 1985\\'\"', 'CARDINAL')\n('\\'One edition calls this Darwin opus one of \"the most readable and approachable\"\" of revolutionary scientific works\\'\"', 'CARDINAL')\n('\\'One edition calls this Darwin opus one of \"the most readable and approachable\"\" of revolutionary scientific works\\'\"', 'CARDINAL')\n(\"'If Joe DiMaggio's hitting streak had gone one more game in 1941, this company would have given him a $10,000 contract'\", 'CARDINAL')\n\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "mydb = connectToMySQL()\n",
    "cursor = mydb.cursor()\n",
    "\n",
    "print('american:')\n",
    "cursor.execute(get_question_by_entity_type % 'american')\n",
    "for r in cursor:\n",
    "    print(str(r))\n",
    "print()\n",
    "\n",
    "print('first:')\n",
    "cursor.execute(get_question_by_entity_type % 'first')\n",
    "for r in cursor:\n",
    "    print(str(r))\n",
    "print()\n",
    "\n",
    "print('one:')\n",
    "cursor.execute(get_question_by_entity_type % 'one')\n",
    "for r in cursor:\n",
    "    print(str(r))\n",
    "print()\n",
    "\n",
    "cursor.close()"
   ]
  },
  {
   "source": [
    "It looks like there are some entity labels that we can rule out. \"ORDINAL\", \"CARDINAL\", and \"NORP\" appear to be presenting an issue already. At this point, one might start to wonder if it is less about entity labels we don't care about, and more about the few we *do* care about. \n",
    "\n",
    "Interestingly, the first appearance of a topic that feels truly \"trivial\" in nature is \"The Clue Crew\" with 363 occurrences in questions. Looking closely to see how it is labeled, \"The Clue Crew\" (the group from the Nancy Drew childrens book series) is an \"ORG\". Glancing through some of the common labels, it looks like we might care about \"PERSON\", \"ORG\" and \"WORK_OF_ART\" at least as a start."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Connected.\n\n('oscar', 356)\n('congress', 213)\n('nyc', 194)\n('the clue crew', 175)\n('shakespeare', 164)\n('senate', 148)\n('jesus', 137)\n('nba', 129)\n('lincoln', 129)\n('nfl', 123)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "mydb = connectToMySQL()\n",
    "cursor = mydb.cursor()\n",
    "\n",
    "cursor.execute(\n",
    "    \"SELECT entity.name, COUNT(*) FROM question_entity_map \" + \n",
    "    \"LEFT JOIN entity ON question_entity_map.entity = entity.name \" +\n",
    "    \"WHERE entity.label IN ('ORG','PERSON','WORK_OF_ART')\" +\n",
    "    \"GROUP BY entity ORDER BY COUNT(*) DESC LIMIT 10;\"\n",
    ")\n",
    "\n",
    "for r in cursor:\n",
    "    print(str(r))\n",
    "\n",
    "cursor.close()"
   ]
  },
  {
   "source": [
    "Looks like someone looking to up their Jeopardy! game (or any well-rounded individual I suppose), should spend some time reading up on the Oscars, the U.S. Congress, NYC, Shakespeare, and Jesus. Or at least, those are some highly frequent entities in questions. This gives a mostly satisfying answer to the original, but leaves a little bit to be desired.\n",
    "\n",
    "Let me point out a couple of problems. \"The Oscars\", again, is a pretty broad category. Might be good to know it is worth going and memorizing, as far as award shows go, but it isn't quite as concrete as \"Lincoln\". Actually most of these results are organizations worth mentioning in a question, but not necessarily the topic of the question.\n",
    "\n",
    "This also doesn't necessarily get at which topics are best to study for Jeopardy!. Some other factors could come into play. As an example, reading all of \"Shakespeare\" is a lot of work, wouldn't it be nice to know that he is only marginally more necessary than \"Harry Potter\", but slightly less niche?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## What next?\n",
    "\n",
    "I hinted at it above, but it looks like there are a couple of fast follows for this analysis.\n",
    "\n",
    "1. We need to figure out how to get to the \"meat\" of what a question is about. That way we can avoid tangential but highly occurring entities. I need to do some research on what is out there, but basically I want to be able to tell what is the central \"Wikipediable\" topic of every question, and count occurrences from there.\n",
    "2. Wouln't it be really nice to know about how niche something is as a piece of data about the topic? If we had that, we could do some kind of composite rank. Highly occurring topics that are less broad would be the easiest to study quickly, thus rewarding the player the most.\n",
    "\n",
    "I'm thinking I'll need to address those ideas in another post. Until then, play on."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}